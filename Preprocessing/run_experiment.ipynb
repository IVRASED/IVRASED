{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "#from datetime import timedelta\n",
    "from hhpy.plotting import animplot\n",
    "\n",
    "import datetime\n",
    "from pandas import datetime as dt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import os, fnmatch\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "\n",
    "# List of groups of columns for EEG, ECG and ET - They are used to run experiments for combination between groups\n",
    "eeg_current_columns1 = ['EEG_Decon_Poz', 'EEG_Decon_P3','EEG_Decon_P4']\n",
    "eeg_current_columns2 = ['EEG_Decon_Cz', 'EEG_Decon_C3', 'EEG_Decon_C4']\n",
    "eeg_current_columns3 = ['EEG_Decon_Fz', 'EEG_Decon_F3', 'EEG_Decon_F4']\n",
    "\n",
    "ecg_columns1 = ['Other_Vsensebatt_RAW_Shim_Exg_02', 'Other_EXG1_Status_RAW_Shim_Exg_02', 'Other_EXG2_Status_RAW_Shim_Exg_02', 'Other_ECG_LL-RA_RAW_Shim_Exg_02', 'Other_ECG_LA-RA_RAW_Shim_Exg_02', 'Other_ECG_Vx-RL_RAW_Shim_Exg_02']\n",
    "ecg_columns2 = ['Other_Vsensebatt_CAL_Shim_Exg_02', 'Other_ECG_LL-RA_CAL_Shim_Exg_02', 'Other_ECG_LA-RA_CAL_Shim_Exg_02', 'Other_ECG_Vx-RL_CAL_Shim_Exg_02']\n",
    "ecg_columns3 = ['Other_Heart_Rate_ECG_LL-RA_ALG_Shim_Exg_02', 'Other_IBI_ECG_LL-RA_ALG_Shim_Exg_02']\n",
    "\n",
    "\n",
    "#et_columns1 = []\n",
    "et_columns1 = ['ET_Gaze_2D_ET_Gazeleftx', 'ET_Gaze_2D_ET_Gazelefty', 'ET_Gaze_2D_ET_Gazerightx', 'ET_Gaze_2D_ET_Gazerighty', 'ET_Capture_ET_Cameraleftx', 'ET_Capture_ET_Cameralefty', 'ET_Capture_ET_Camerarightx', 'ET_Capture_ET_Camerarighty', 'ET_Head_ET_Headrotationx', 'ET_Head_ET_Headrotationy', 'ET_Head_ET_Headrotationz', 'ET_Head_ET_Headpositionvectorx', 'ET_Head_ET_Headpositionvectory', 'ET_Head_ET_Headpositionvectorz','ET_Head_ET_Headvelocityx', 'ET_Head_ET_Headvelocityy', 'ET_Head_ET_Headvelocityz','ET_Head_ET_Headangularvelocityx', 'ET_Head_ET_Headangularvelocityy','ET_Head_ET_Headangularvelocityz', 'ET_Other_ET_Timesignal', 'ET_Distance_ET_Distanceleft','ET_Distance_ET_Distanceright']\n",
    "et_columns2 = ['ET_Pupil_ET_Pupilleft', 'ET_Expression_ET_Lefteyeopenness''ET_Expression_ET_Lefteyesqueeze','ET_Expression_ET_Lefteyefrown']\n",
    "et_columns3 = ['ET_Pupil_ET_Pupilright', 'ET_Expression_ET_Righteyesqueeze', 'ET_Expression_ET_Righteyeopenness', 'ET_Expression_ET_Righteyefrown']\n",
    "              \n",
    "\n",
    "all_columns_groups = {'eeg_current_columns1':eeg_current_columns1, 'eeg_current_columns2':eeg_current_columns2, 'eeg_current_columns3':eeg_current_columns3, 'ecg_columns1':ecg_columns1, 'ecg_columns2':ecg_columns2, 'ecg_columns3':ecg_columns3, 'et_columns1':et_columns1, 'et_columns2':et_columns2, 'et_columns3':et_columns3}\n",
    "\n",
    "\n",
    "all_columns = {'B-Alert Decontaminated EEG' : {'eeg_current_columns1':eeg_current_columns1, 'eeg_current_columns2':eeg_current_columns2, 'eeg_current_columns3':eeg_current_columns3},\n",
    "               'Eyetracker HTC VIVE Pro Eye' : {'et_columns1':et_columns1, 'et_columns2':et_columns2, 'et_columns3':et_columns3},\n",
    "                'Shimmer shim exg 02 5F2F ECG': { 'ecg_columns1':ecg_columns1, 'ecg_columns2':ecg_columns2, 'ecg_columns3':ecg_columns3}\n",
    "              }\n",
    "\n",
    "groups = []\n",
    "for i in all_columns:\n",
    "    groups = groups + list(all_columns[i].keys())\n",
    "\n",
    "# return the name of the list of groups\n",
    "def get_key(val):\n",
    "    for key, value in all_columns.items():\n",
    "        if val in value:\n",
    "            return key\n",
    " \n",
    "    return \"key doesn't exist\"\n",
    " \n",
    "\n",
    "groupstoprint = []\n",
    "for i in all_columns:\n",
    "    #print(all_columns[i].keys())\n",
    "    #print(i)\n",
    "    for j in all_columns[i]:\n",
    "        print(j, end=',')\n",
    "    for k in all_columns[i]:\n",
    "        for f in all_columns[i][k]:\n",
    "            print(f)\n",
    "    groupstoprint = groupstoprint + list(all_columns[i].keys())\n",
    "    \n",
    "    \n",
    "\n",
    "# print the number of each class in dataset\n",
    "def countClasses(y_train):\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "\n",
    "# manage the number of classes for experiments - you should modify this function if you need the create multiple classes\n",
    "def manageClasses(all_Y):\n",
    "    \n",
    "    all_Y = np.where(all_Y <=7, 0, all_Y)\n",
    "    all_Y = np.where(all_Y >7 , 1, all_Y)\n",
    "  \n",
    "    #all_Y = np.where(all_Y <=6, 0, all_Y)\n",
    "    #all_Y = np.where(all_Y ==7 , 1, all_Y)\n",
    "    #all_Y = np.where(all_Y ==8 , 1, all_Y)\n",
    "    #all_Y = np.where(all_Y >8, 2, all_Y)\n",
    "\n",
    "    #all_Y = np.where(all_Y ==10, 3, all_Y)\n",
    "    \n",
    "\n",
    "    countClasses(all_Y)\n",
    "    \n",
    "    return all_Y\n",
    "\n",
    "def prepare3DArray(data, all_data_by_event, workedData, events = ['B-Alert Decontaminated EEG', 'B-Alert EEG',   'Eyetracker HTC VIVE Pro Eye','R Analysis GazeAnalysis I-VT filter','Shimmer GSR 4B59', 'Shimmer shim exg 02 5F2F ECG']):       \n",
    "    #events = ['Eyetracker HTC VIVE Pro Eye']\n",
    "    rownb = 0\n",
    "    shapes = []\n",
    "    #workedData = []\n",
    "    with_problemData = []\n",
    "    \n",
    "    for i in range(len(data)):       \n",
    "        rownb = rownb + data[i].shape[0]\n",
    "        data[i] = data[i].dropna()\n",
    "        #print(i)\n",
    "        #print(data[i]['class'])\n",
    "        if data[i]['class'].empty:\n",
    "            print('DataFrame is empty! ', i)\n",
    "        else:\n",
    "            #if data[i]['class'][0] < 10:\n",
    "                #print(data[i].shape[1])\n",
    "            if data[i].shape[0]/128 >= 2:\n",
    "                #print(data[i].shape[0])\n",
    "                class_data = int(data[i].iloc[1,-1:])\n",
    "                dataTranspose = data[i].tail(256).iloc[:,:-1].T\n",
    "                dataTranspose['class'] = class_data\n",
    "                if(data[i].shape[1] <46):\n",
    "                    with_problemData.append(dataTranspose.values)\n",
    "                else:\n",
    "                    workedData.append(dataTranspose.values)\n",
    "\n",
    "                for idx, key in enumerate(events):\n",
    "                    events_needed = [key]\n",
    "                    # filter data by the columns of current event\n",
    "                    if key in data[i].columns:\n",
    "                        singleEventData = data[i][key]\n",
    "                        dataEventTranspose = singleEventData.tail(256).T\n",
    "                        # Check if all 2D numpy array contains only 0\n",
    "                        result = np.all((dataEventTranspose.values == 0))\n",
    "                        if result:\n",
    "                            print('2D Array contains only 0 i=',i, ' key = '+key)\n",
    "                            #print(dataEventTranspose)\n",
    "                        else:\n",
    "                        #    print('2D Array has non-zero items too')\n",
    "                            dataEventTranspose['class'] = class_data\n",
    "                            if key not in all_data_by_event:\n",
    "                                all_data_by_event[key] = [] #dataTranspose.values\n",
    "                                #print(\"not in\")\n",
    "                            #else:\n",
    "                                #np.append(all_data_by_event[key],dataTranspose.values)\n",
    "                                #print(\"in\")\n",
    "                            #print(dataTranspose.shape)\n",
    "                            all_data_by_event[key].append(dataEventTranspose.values)\n",
    "                        #print(key+ \" i = \"+str(i))\n",
    "                        #print(np.array(all_data_by_event[key]).shape)\n",
    "\n",
    "\n",
    "    #print(rownb)\n",
    "    #print(np.min(shapes))\n",
    "    #print(np.max(shapes))\n",
    "    return workedData, all_data_by_event\n",
    "\n",
    "# check if data of selected events is valid and has values\n",
    "def isValidData(data, eventsCombination):\n",
    "    isValid = True\n",
    "    for oneEvent in eventsCombination:\n",
    "        combinationOneEventData = data[oneEvent]\n",
    "        isOneEventZeros = np.all((combinationOneEventData.values == 0))\n",
    "\n",
    "        if isOneEventZeros:\n",
    "            return False\n",
    "\n",
    "    nan_Nb = data.isnull().sum().sum()\n",
    "    if nan_Nb >0:\n",
    "        return False\n",
    "    else:\n",
    "        # Check if all 2D numpy array contains only 0\n",
    "        result = np.all((data.values == 0))\n",
    "        if result:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "            dataEventsTranspose['class'] = class_data\n",
    "            combination_name = '_'.join(eventsCombination)\n",
    "            if combination_name not in all_data_by_event_combination:\n",
    "                all_data_by_event_combination[combination_name] = [] #dataTranspose.values\n",
    "                #print(\"not in\")\n",
    "\n",
    "\n",
    "        all_data_by_event_combination[combination_name].append(dataEventsTranspose.values)\n",
    "\n",
    "#cut data based window slicing method - with overlap or without, based on parameters values\n",
    "#window_size_in_sec - correspond to the size of one window in seconds ex: 2 seconds\n",
    "#data_to_slice_full_time - correspond to the size of data to cut into set of windows ex: 10 seconds (the last 10s to cut from the given data)\n",
    "#slice_size - correspond to the overlap between windows ex:0.5s \n",
    "#sample_rate - correspond to the number of rows per second ex:128\n",
    "def getDataByWindowsSlicing(data, window_size_in_sec=2, data_to_slice_full_time=2, slice_size=0, sample_rate=128):\n",
    "    dataArray = []\n",
    "    \n",
    "    window_size_by_frame = window_size_in_sec*sample_rate\n",
    "    data_to_slice_full_time_by_frame = data_to_slice_full_time*sample_rate\n",
    "    slice_size_by_frame = slice_size*sample_rate\n",
    "    \n",
    "\n",
    "    current_window_begin = window_size_by_frame\n",
    "    current_window_end = 0\n",
    "    data_size = data.shape[0]\n",
    "\n",
    "    #check if data size is less then the data to slice, in order to get windows for existing data only\n",
    "    if( data_size < data_to_slice_full_time_by_frame):\n",
    "        data_to_slice_full_time_by_frame = data_size\n",
    "    \n",
    "    count = 0\n",
    "    # loop data until it cut all windows in the full slice\n",
    "    while (current_window_begin<=data_to_slice_full_time_by_frame):\n",
    "        if current_window_end==0:\n",
    "            current_window_data = data.iloc[-current_window_begin:]\n",
    "        else:\n",
    "            current_window_data = data.iloc[-current_window_begin:-current_window_end]\n",
    "\n",
    "        dataArray.append(current_window_data)\n",
    "        count +=1\n",
    "        current_window_begin = int(current_window_begin+slice_size_by_frame)\n",
    "        current_window_end = int(current_window_end+slice_size_by_frame)\n",
    "    return dataArray\n",
    "\n",
    "# remove data with empty classes\n",
    "def filterData(data):\n",
    "    filteredData = []\n",
    "    for i in range(len(data)):\n",
    "        #print(data[i]['class'][0])\n",
    "        if data[i]['class'].empty:\n",
    "            print(' DataFrame is empty! ', i)\n",
    "        else:\n",
    "            filteredData.append(data[i])\n",
    "    return filteredData\n",
    "\n",
    "#return dataset for the given combination\n",
    "def prepare3DArrayByCombinationAndFeatures(data, all_data_by_event_combination, eventsCombination, current_features_to_use):       \n",
    "    global window_size_in_sec, data_to_slice_full_time, slice_size, sample_rate\n",
    "\n",
    "    #loop each SOC data\n",
    "    for i in range(len(data)):\n",
    "        #check that data is valid\n",
    "        if data[i]['class'].empty:\n",
    "            print(' DataFrame is empty! ', i)\n",
    "        else:\n",
    "            #check that data has more than 2 seconds\n",
    "            if data[i].shape[0]/128 >= 2:\n",
    "                #read class for the current SOC\n",
    "                class_data = int(data[i].iloc[1,-1:])\n",
    "\n",
    "                #Get data only if all events combination exist\n",
    "                if(all(x in list(data[i].columns.levels[0]) for x in eventsCombination)): \n",
    "                    \n",
    "\n",
    "                        \n",
    "                    combination_name = '_'.join(current_features_to_use)\n",
    "\n",
    "                    CombinationEventData = data[i].loc[:,(eventsCombination,current_features_to_use)]\n",
    "                    #cut data to windows based on selected parameters\n",
    "                    dataArray = getDataByWindowsSlicing(CombinationEventData, window_size_in_sec=window_size_in_sec, data_to_slice_full_time=data_to_slice_full_time, slice_size=slice_size, sample_rate=sample_rate)\n",
    "                    \n",
    "                    #loop each window of data, and add it to the list of data of current combination\n",
    "                    for dataWindow in dataArray:\n",
    "                        if isValidData(dataWindow, eventsCombination):\n",
    "                            dataEventsTranspose = dataWindow.T\n",
    "                            dataEventsTranspose['class'] = class_data\n",
    "                            if combination_name not in all_data_by_event_combination:\n",
    "                                all_data_by_event_combination[combination_name] = [] #dataTranspose.values\n",
    "                        \n",
    "                            all_data_by_event_combination[combination_name].append(dataEventsTranspose.values)\n",
    "                    \n",
    "    return all_data_by_event_combination\n",
    "\n",
    "def prepare3DArrayByCombination(data, all_data_by_event_combination, eventsCombination = ['B-Alert Decontaminated EEG', 'B-Alert EEG',   'Eyetracker HTC VIVE Pro Eye','R Analysis GazeAnalysis I-VT filter','Shimmer GSR 4B59', 'Shimmer shim exg 02 5F2F ECG']):       \n",
    "    global window_size_in_sec, data_to_slice_full_time, slice_size, sample_rate\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        #print('data ',i)\n",
    "        if data[i]['class'].empty:\n",
    "            print(' DataFrame is empty! ', i)\n",
    "        else:\n",
    "            #if data[i]['class'][0] < 10:\n",
    "                #print(data[i].shape[1])\n",
    "            if data[i].shape[0]/128 >= 2:\n",
    "                #print(data[i].shape[0])\n",
    "                class_data = int(data[i].iloc[1,-1:])\n",
    "                #if eventsCombination in data[i]:\n",
    "\n",
    "                #Get data only if all events combination exist\n",
    "                if(all(x in list(data[i].columns.levels[0]) for x in eventsCombination)): \n",
    "                    \n",
    "\n",
    "                        \n",
    "                    combination_name = '_'.join(eventsCombination)\n",
    "                    #print(combination_name)\n",
    "                    CombinationEventData = data[i][eventsCombination]\n",
    "                    dataArray = getDataByWindowsSlicing(CombinationEventData, window_size_in_sec=window_size_in_sec, data_to_slice_full_time=data_to_slice_full_time, slice_size=slice_size, sample_rate=sample_rate)         \n",
    "                    for dataWindow in dataArray:\n",
    "                        if isValidData(dataWindow, eventsCombination):\n",
    "                            dataEventsTranspose = dataWindow.T\n",
    "                            dataEventsTranspose['class'] = class_data\n",
    "                            if combination_name not in all_data_by_event_combination:\n",
    "                                all_data_by_event_combination[combination_name] = [] #dataTranspose.values\n",
    "                        \n",
    "                            all_data_by_event_combination[combination_name].append(dataEventsTranspose.values)\n",
    "                    \n",
    "    return all_data_by_event_combination\n",
    "\n",
    "def prepare3DArrayByCombination_initial(data, all_data_by_event_combination, eventsCombination = ['B-Alert Decontaminated EEG', 'B-Alert EEG',   'Eyetracker HTC VIVE Pro Eye','R Analysis GazeAnalysis I-VT filter','Shimmer GSR 4B59', 'Shimmer shim exg 02 5F2F ECG']):       \n",
    "    for i in range(len(data)):       \n",
    "        if data[i]['class'].empty:\n",
    "            print('DataFrame is empty! ', i)\n",
    "        else:\n",
    "            #if data[i]['class'][0] < 10:\n",
    "                #print(data[i].shape[1])\n",
    "            if data[i].shape[0]/128 >= 2:\n",
    "                #print(data[i].shape[0])\n",
    "                class_data = int(data[i].iloc[1,-1:])\n",
    "                #if eventsCombination in data[i]:\n",
    "\n",
    "                if(all(x in list(data[i].columns.levels[0]) for x in eventsCombination)): \n",
    "                    \n",
    "\n",
    "                        \n",
    "\n",
    "                #if data[i].columns.isin(eventsCombination, level=0).all():\n",
    "                    #print(eventsCombination)\n",
    "                    CombinationEventData = data[i][eventsCombination]\n",
    "                    \n",
    "                    \n",
    "                    #dataEvents = CombinationEventData.tail(256).iloc[:,:-1]\n",
    "                    dataEvents = CombinationEventData.tail(256)\n",
    "                    #print(CombinationEventData.shape, ' : ', dataEvents.shape)\n",
    "\n",
    "                    for oneEvent in eventsCombination:\n",
    "                        combinationOneEventData = dataEvents[oneEvent]\n",
    "                        isOneEventZeros = np.all((combinationOneEventData.values == 0))\n",
    "                        #print('isOneEventZeros ', isOneEventZeros)\n",
    "                        #print(combinationOneEventData)\n",
    "                        if isOneEventZeros==True:\n",
    "                            print(\"you should skip this post\")\n",
    "                        to_skip = False\n",
    "                        if isOneEventZeros:\n",
    "                            #print('2D Array contains of one event only 0 i=',i, ' event = '+oneEvent)\n",
    "                            to_skip = True\n",
    "                            #print(dataEventTranspose)\n",
    "                        #else:\n",
    "                    #dataEventsTranspose = CombinationEventData.tail(256).iloc[:,:-1].T\n",
    "                    dataEventsTranspose = dataEvents.T\n",
    "\n",
    "                    nan_Nb = dataEventsTranspose.isnull().sum().sum()\n",
    "                    if nan_Nb >0 or to_skip:\n",
    "                        #print(nan_Nb, '     to skip:',)\n",
    "                        dataEventsTranspose = dataEventsTranspose.dropna()\n",
    "                    else:\n",
    "                        # Check if all 2D numpy array contains only 0\n",
    "                        #print(dataEventsTranspose.columns)\n",
    "                        result = np.all((dataEventsTranspose.values == 0))\n",
    "                        if result:\n",
    "                            print('2D Array contains only 0 i=',i, ' Events = ',eventsCombination)\n",
    "                            #print(dataEventTranspose)\n",
    "                        else:\n",
    "                        #    print('2D Array has non-zero items too')\n",
    "                            dataEventsTranspose['class'] = class_data\n",
    "                            combination_name = '_'.join(eventsCombination)\n",
    "                            if combination_name not in all_data_by_event_combination:\n",
    "                                all_data_by_event_combination[combination_name] = [] #dataTranspose.values\n",
    "                                #print(\"not in\")\n",
    "                        \n",
    "                        \n",
    "                        all_data_by_event_combination[combination_name].append(dataEventsTranspose.values)\n",
    "    return all_data_by_event_combination\n",
    "\n",
    "def prepare3DArrayByCombination_old(data, all_data_by_event_combination, eventsCombination = ['B-Alert Decontaminated EEG', 'B-Alert EEG',   'Eyetracker HTC VIVE Pro Eye','R Analysis GazeAnalysis I-VT filter','Shimmer GSR 4B59', 'Shimmer shim exg 02 5F2F ECG']):       \n",
    "    for i in range(len(data)):       \n",
    "        if data[i]['class'].empty:\n",
    "            print('DataFrame is empty! ', i)\n",
    "        else:\n",
    "            #if data[i]['class'][0] < 10:\n",
    "                #print(data[i].shape[1])\n",
    "            if data[i].shape[0]/128 >= 2:\n",
    "                #print(data[i].shape[0])\n",
    "                class_data = int(data[i].iloc[1,-1:])\n",
    "                #if eventsCombination in data[i]:\n",
    "\n",
    "                if(all(x in list(data[i].columns.levels[0]) for x in eventsCombination)): \n",
    "\n",
    "                #if data[i].columns.isin(eventsCombination, level=0).all():\n",
    "                    #print(eventsCombination)\n",
    "                    CombinationEventData = data[i][eventsCombination]\n",
    "\n",
    "                    #dataEventsTranspose = CombinationEventData.tail(256).iloc[:,:-1].T\n",
    "                    dataEventsTranspose = CombinationEventData.tail(256).T\n",
    "                    # Check if all 2D numpy array contains only 0\n",
    "                    result = np.all((dataEventsTranspose.values == 0))\n",
    "                    if result:\n",
    "                        print('2D Array contains only 0 i=',i, ' key = '+key)\n",
    "                        #print(dataEventTranspose)\n",
    "                    else:\n",
    "                    #    print('2D Array has non-zero items too')\n",
    "                        dataEventsTranspose['class'] = class_data\n",
    "                        combination_name = '_'.join(eventsCombination)\n",
    "                        if combination_name not in all_data_by_event_combination:\n",
    "                            all_data_by_event_combination[combination_name] = [] #dataTranspose.values\n",
    "                            #print(\"not in\")\n",
    "                        all_data_by_event_combination[combination_name].append(dataEventsTranspose.values)\n",
    "\n",
    "    return all_data_by_event_combination\n",
    "def loadDataFromSavedArraysByUser():\n",
    "    all_data_by_user = {}\n",
    "    #sequence_list = []\n",
    "    path = '/home/deep01/zaher/exportedObject/normal/'\n",
    "    listOfFiles = os.listdir(path)\n",
    "    pattern = \"*_data.array\"\n",
    "    for entry in listOfFiles:\n",
    "        if fnmatch.fnmatch(entry, pattern):\n",
    "            userId = entry.split('_')[0];\n",
    "            #print (userId)\n",
    "            if userId not in all_data_by_user:\n",
    "                all_data_by_user[userId] = [] \n",
    "                \n",
    "            single_data = pd.read_pickle(path+entry)\n",
    "            #print(len(single_data))\n",
    "\n",
    "            all_data_by_user[userId] = np.concatenate((all_data_by_user[userId],single_data))\n",
    "    return all_data_by_user\n",
    "            \n",
    "def loadDataFromSavedArrays():\n",
    "    data = pd.read_pickle(\"/home/deep01/zaher/all_data_full_features_10_classes.array\")\n",
    "    data.shape\n",
    "    data1 = pd.read_pickle(\"/home/deep01/zaher/all_data_full_features_with_problem_10_classes.array\")\n",
    "    data1.shape\n",
    "    data2 = pd.read_pickle(\"/home/deep01/zaher/all_data_full_features_exception_10_classes.array\")\n",
    "    data2.shape\n",
    "    return data, data1, data2\n",
    "\n",
    "\n",
    "def prepareAllDataArrays(data, data1, data2):\n",
    "    all_data_by_event = {}\n",
    "    workedData = []\n",
    "    workedData, all_data_by_event = prepare3DArray(data,all_data_by_event, workedData)\n",
    "    workedData, all_data_by_event = prepare3DArray(data1,all_data_by_event, workedData)\n",
    "    workedData, all_data_by_event = prepare3DArray(data2, all_data_by_event, workedData)\n",
    "    \n",
    "def prepareAllDataArraysByCombination(data, data1, data2):\n",
    "    all_data_by_event_combination = {}\n",
    "    events = ['B-Alert Decontaminated EEG', 'B-Alert EEG',   'Eyetracker HTC VIVE Pro Eye','R Analysis GazeAnalysis I-VT filter','Shimmer GSR 4B59', 'Shimmer shim exg 02 5F2F ECG']\n",
    "    count = 0\n",
    "    for L in range(1,len(events)+1):\n",
    "        for subset in itertools.combinations(events, L):\n",
    "            eventsCombination = list(subset)\n",
    "            #print(eventsCombination, \"  :  \", len(eventsCombination))\n",
    "            all_data_by_event_combination = prepare3DArrayByCombination(data, all_data_by_event_combination, eventsCombination)\n",
    "            all_data_by_event_combination = prepare3DArrayByCombination(data1, all_data_by_event_combination, eventsCombination)\n",
    "            all_data_by_event_combination = prepare3DArrayByCombination(data2, all_data_by_event_combination, eventsCombination)\n",
    "    \n",
    "    return all_data_by_event_combination\n",
    "\n",
    "# return a list of dataset for all combination of events\n",
    "def prepareAllSplitedDataArraysByCombination(data):\n",
    "    all_data_by_event_combination = {}\n",
    "    events = ['B-Alert Decontaminated EEG', 'B-Alert EEG',   'Eyetracker HTC VIVE Pro Eye','R Analysis GazeAnalysis I-VT filter','Shimmer GSR 4B59', 'Shimmer shim exg 02 5F2F ECG']\n",
    "    count = 0\n",
    "    for L in range(1,len(events)+1):\n",
    "        for subset in itertools.combinations(events, L):\n",
    "            eventsCombination = list(subset)\n",
    "            print(eventsCombination, \"  :  \", len(eventsCombination))\n",
    "            all_data_by_event_combination = prepare3DArrayByCombination(data, all_data_by_event_combination, eventsCombination)\n",
    "\n",
    "    return all_data_by_event_combination\n",
    "\n",
    "# return a list of dataset for all combination of groups\n",
    "def prepareAllSplitedDataArraysByCombinationAndFeatures(data, groups):\n",
    "    all_data_by_event_combination_features = {}\n",
    "    count = 0\n",
    "    # loop al combination\n",
    "    for L in range(2,len(groups)+1):\n",
    "        for subset in itertools.combinations(groups, L):\n",
    "            # list of one combination\n",
    "            eventsCombination = list(subset)\n",
    "            current_columns = []\n",
    "            current_combination_columns_groups = {}\n",
    "            #create the list columns for this combination\n",
    "            for oneColumnsSet in eventsCombination:\n",
    "                currentEvent = get_key(oneColumnsSet)\n",
    "                if currentEvent in current_combination_columns_groups:\n",
    "                    current_combination_columns_groups[currentEvent] = current_combination_columns_groups[currentEvent] + all_columns_groups[oneColumnsSet]\n",
    "                else:\n",
    "                    current_combination_columns_groups[currentEvent] = all_columns_groups[oneColumnsSet]\n",
    "\n",
    "                current_columns =current_columns+  all_columns_groups[oneColumnsSet]\n",
    "            #return dataset for the current combination\n",
    "            all_data_by_event_combination_features = prepare3DArrayByCombinationAndFeatures(data, all_data_by_event_combination_features, list(current_combination_columns_groups.keys()), current_columns) \n",
    "\n",
    "            print(count)\n",
    "            print(current_columns)\n",
    "            print(\"---------------------------------------------------------------------\")\n",
    "            count +=1\n",
    "\n",
    "\n",
    "    return all_data_by_event_combination_features\n",
    "\n",
    "def prepareAllDataArraysByOneCombination(data, data1, data2, eventsCombination):\n",
    "    all_data_by_event_combination = {}\n",
    "\n",
    "    all_data_by_event_combination = prepare3DArrayByCombination(data, all_data_by_event_combination, eventsCombination)\n",
    "    all_data_by_event_combination = prepare3DArrayByCombination(data1, all_data_by_event_combination, eventsCombination)\n",
    "    all_data_by_event_combination = prepare3DArrayByCombination(data2, all_data_by_event_combination, eventsCombination)\n",
    "\n",
    "    return all_data_by_event_combination\n",
    "\n",
    "def exportMLSTMData(X, Y, path):\n",
    "    global data_folder_name\n",
    "    initial_path = \"/home/deep01/zaher/MLSTM-FCN-master/\"+data_folder_name+\"/\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    #y_train = y_train\n",
    "    #y_test = y_test[:,0]\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    # ''' Save the datasets '''\n",
    "    #print(\"Train dataset : \", X_train.shape, y_train.shape)\n",
    "    #print(\"Test dataset : \", X_test.shape, y_test.shape)\n",
    "    #print(\"Train dataset metrics : \", X_train.mean(), X_train.std())\n",
    "    #print(\"Test dataset : \", X_test.mean(), X_test.std())\n",
    "    #print(\"Nb classes : \", len(np.unique(y_train)))\n",
    "    \n",
    "    countClasses(y_train)\n",
    "    \n",
    "    countClasses(y_test)\n",
    "    \n",
    "    path = initial_path+path\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    #print(path)\n",
    "    np.save(path + '/X_train.npy', X_train)\n",
    "    np.save(path + '/y_train.npy', y_train)\n",
    "    np.save(path + '/X_test.npy', X_test)\n",
    "    np.save(path + '/y_test.npy', y_test)\n",
    "    \n",
    "def exportMLSTMDataWithoutSplit(X, Y, path, Datatype= \"train\"):\n",
    "    global data_folder_name\n",
    "    initial_path = \"/home/deep01/zaher/MLSTM-FCN-master/\"+data_folder_name+\"/\"\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    #y_train = y_train\n",
    "    #y_test = y_test[:,0]\n",
    "    Y = Y.reshape(Y.shape[0],1)\n",
    "    #y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    # ''' Save the datasets '''\n",
    "    #print(Datatype+\" Train dataset : \", X.shape, Y.shape)\n",
    "    #print(Datatype+\" dataset metrics : \", X.mean(), X.std())\n",
    "    #print(\"Nb classes : \", len(np.unique(Y)))\n",
    "    \n",
    "    countClasses(Y)\n",
    "    \n",
    "    #countClasses(y_test)\n",
    "    \n",
    "    path = initial_path+path\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    #print(path)\n",
    "    np.save(path + '/X_'+Datatype+'.npy', X)\n",
    "    np.save(path + '/y_'+Datatype+'.npy', Y)\n",
    "    #np.save(path + '/X_test.npy', X_test)\n",
    "    #np.save(path + '/y_test.npy', y_test)\n",
    "\n",
    "def exportDataByevent(all_data_by_event):\n",
    "    for idx, key in enumerate(all_data_by_event.keys()):\n",
    "        #print(key)\n",
    "        #print(np.array(all_data_by_event[key]).shape)\n",
    "        \n",
    "        workedDataNP = np.array(all_data_by_event[key])\n",
    "        X = workedDataNP[:,:,:-1]\n",
    "        Y = workedDataNP[:,0,-1]\n",
    "        #print(X.shape)\n",
    "        #print(Y.shape)\n",
    "        #print(Y)\n",
    "        Y = manageClasses(Y)\n",
    "        exportMLSTMData(X, Y, path=key)\n",
    "\n",
    "def exportDataByeventWithoutSplit(all_data_by_event, Datatype=\"train\"):\n",
    "    for idx, key in enumerate(all_data_by_event.keys()):\n",
    "        #print(key)\n",
    "        #print(np.array(all_data_by_event[key]).shape)\n",
    "        \n",
    "        workedDataNP = np.array(all_data_by_event[key])\n",
    "        X = workedDataNP[:,:,:-1]\n",
    "        Y = workedDataNP[:,0,-1]\n",
    "        \n",
    "        #indexArr = np.argwhere(Y == 10)\n",
    "        #print(X.shape)\n",
    "        #print(Y.shape)\n",
    "        \n",
    "        #Y = np.delete(Y, indexArr)\n",
    "        #X = np.delete(X, indexArr, axis=0)\n",
    "\n",
    "        #print(X.shape)\n",
    "        #print(Y.shape)\n",
    "\n",
    "        #print(Y)\n",
    "        Y = manageClasses(Y)\n",
    "        #path = key\n",
    "        path = \"model\"+str(idx)\n",
    "        exportMLSTMDataWithoutSplit(X, Y, path=path, Datatype=Datatype)\n",
    "    \n",
    "def exportDataAllCompleteData(workedData):\n",
    "    workedDataNP = np.array(workedData)\n",
    "    #print(workedDataNP.shape)\n",
    "    X = workedDataNP[:,:,:-1]\n",
    "    Y = workedDataNP[:,0,-1]\n",
    "    \n",
    "    #print(X.shape)\n",
    "    #print(Y.shape)\n",
    "    #print(Y)\n",
    "    Y = manageClasses(Y)\n",
    "    exportMLSTMData(X, Y, path=\"zaher\")\n",
    "\n",
    "def splitData(data, Train_rate = 0.8):\n",
    "    AllfilterData = filterData(data)\n",
    "    trainingSize = int(len(AllfilterData)*0.8)\n",
    "    #print(trainingSize)\n",
    "    np.random.shuffle(AllfilterData)\n",
    "    return AllfilterData[:trainingSize], AllfilterData[trainingSize:]\n",
    "\n",
    "\n",
    "# create a dataset (train and test) ready for deep learning model - with all combination of events\n",
    "# You can creade many folds for the same dataset randomly splitted using \"nb_fold\" parameter\n",
    "# \n",
    "def crossValSplitForEventCombination(nb_fold, data, folder_name_prefix=\"_data_no_augmentation_2C_byPost_all_new\"):\n",
    "    global data_folder_name\n",
    "    export_path = '/home/deep01/zaher/MLSTM-FCN-master/output/'\n",
    "    for i in range(nb_fold):\n",
    "        print(\"fold \"+str(i))\n",
    "        # split this fold randomly to train and test\n",
    "        training, test = splitData(data, Train_rate = 0.8)\n",
    "        \n",
    "        # return a list of train dataset for all events combination\n",
    "        all_training_by_event_combination_n = prepareAllSplitedDataArraysByCombination(training)\n",
    "        # return a list of test dataset for all events combination\n",
    "        all_test_by_event_combination_n = prepareAllSplitedDataArraysByCombination(test)\n",
    "        \n",
    "        # export each combination in a repository for this fold(train and test together)\n",
    "        data_folder_name = \"output/fold\"+str(i)+folder_name_prefix\n",
    "        exportDataByeventWithoutSplit(all_training_by_event_combination_n, \"train\")\n",
    "        exportDataByeventWithoutSplit(all_test_by_event_combination_n, \"test\")\n",
    "        \n",
    "        # save both lists in a pickle\n",
    "        pickleName =  'all_training_by_event_combination_overlap_fold'+str(i)+'.array'\n",
    "        pickleFullPath = export_path + pickleName\n",
    "        with open(pickleFullPath, 'wb') as all_training_by_event_combination_file:\n",
    "            pickle.dump(all_training_by_event_combination_n, all_training_by_event_combination_file)\n",
    "            \n",
    "        pickleName =  'all_test_by_event_combination_overlap_fold'+str(i)+'.array'\n",
    "        pickleFullPath = export_path + pickleName\n",
    "        with open(pickleFullPath, 'wb') as all_test_by_event_combination_file:\n",
    "            pickle.dump(all_test_by_event_combination_n, all_test_by_event_combination_file)\n",
    "            \n",
    "\n",
    "# create a dataset (train and test) ready for deep learning model - with all combination of groups of features\n",
    "# You can creade many folds for the same dataset randomly splitted using \"nb_fold\" parameter\n",
    "# \n",
    "def crossValSplitForFeaturesCombination(nb_fold, data, groups, folder_name_prefix=\"_data_no_augmentation_2C_byPost_all_new\"):\n",
    "    global data_folder_name\n",
    "    export_path = '/home/deep01/zaher/MLSTM-FCN-master/output/'\n",
    "    for i in range(nb_fold):\n",
    "        print(\"fold \"+str(i))\n",
    "        # split this fold randomly to train and test\n",
    "        training, test = splitData(data, Train_rate = 0.8)\n",
    "        \n",
    "        # return a list of train dataset for all combination\n",
    "        all_training_by_event_combination_n = prepareAllSplitedDataArraysByCombinationAndFeatures(training, groups)\n",
    "        # return a list of test dataset for all combination\n",
    "        all_test_by_event_combination_n = prepareAllSplitedDataArraysByCombinationAndFeatures(test, groups)\n",
    "        \n",
    "        # export each combination in a repository for this fold(train and test together)\n",
    "        data_folder_name = \"output/fold\"+str(i)+folder_name_prefix\n",
    "        exportDataByeventWithoutSplit(all_training_by_event_combination_n, \"train\")\n",
    "        exportDataByeventWithoutSplit(all_test_by_event_combination_n, \"test\")\n",
    "        \n",
    "        # save both lists in a pickle\n",
    "        pickleName =  'all_training_by_event_combination_by_features_overlap_'+str(folder_name_prefix)+'_fold'+str(i)+'.array'\n",
    "        pickleFullPath = export_path + pickleName\n",
    "        with open(pickleFullPath, 'wb') as all_training_by_event_combination_file:\n",
    "            pickle.dump(all_training_by_event_combination_n, all_training_by_event_combination_file)\n",
    "            \n",
    "        pickleName =  'all_test_by_event_combination_by_features_overlap_'+str(folder_name_prefix)+'fold'+str(i)+'.array'\n",
    "        pickleFullPath = export_path + pickleName\n",
    "        with open(pickleFullPath, 'wb') as all_test_by_event_combination_file:\n",
    "            pickle.dump(all_test_by_event_combination_n, all_test_by_event_combination_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Read data exported from pre_processing.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllData = pd.read_pickle(\"/home/deep01/zaher/all_data_full_features_10_classes_final.array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - add your parameters for the data to export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of parameters to export data(train and test) without overlap - take only last 2 seconds of each SEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_in_sec=2\n",
    "data_to_slice_full_time=2\n",
    "slice_size=0.5\n",
    "sample_rate=128\n",
    "data_folder_name = \"\"\n",
    "folds_number = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of parameters to export data(train and test) with overlap - For each SEF, take last 10 seconds and compose it to a set of windows of 2 seconds each with 0,5 of overlap between each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_in_sec=2\n",
    "data_to_slice_full_time=10\n",
    "slice_size=0.5\n",
    "sample_rate=128\n",
    "data_folder_name = \"\"\n",
    "folds_number = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Run the file to make all combination of groups with an n folds for each combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run all combination of the groups of features (article 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossValSplitForFeaturesCombination(folds_number, AllData, groups, folder_name_prefix= \"_data_with_augmentation_2C_byFeatures_all_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run all combination of the events (article 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossValSplitForEventCombination(folds_number, AllData, folder_name_prefix= \"_data_with_augmentation_2C_byEvents_all_new\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
